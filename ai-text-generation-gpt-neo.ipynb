{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-21T12:35:12.873961Z","iopub.execute_input":"2022-11-21T12:35:12.874724Z","iopub.status.idle":"2022-11-21T12:35:12.897163Z","shell.execute_reply.started":"2022-11-21T12:35:12.874622Z","shell.execute_reply":"2022-11-21T12:35:12.896338Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!wget -O prompts.csv https://raw.githubusercontent.com/krea-ai/open-prompts/main/data/1k.csv","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:12.912075Z","iopub.execute_input":"2022-11-21T12:35:12.912345Z","iopub.status.idle":"2022-11-21T12:35:14.823763Z","shell.execute_reply.started":"2022-11-21T12:35:12.912321Z","shell.execute_reply":"2022-11-21T12:35:14.822600Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2022-11-21 12:35:13--  https://raw.githubusercontent.com/krea-ai/open-prompts/main/data/1k.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1027887 (1004K) [text/plain]\nSaving to: ‘prompts.csv’\n\nprompts.csv         100%[===================>]   1004K  --.-KB/s    in 0.06s   \n\n2022-11-21 12:35:14 (16.9 MB/s) - ‘prompts.csv’ saved [1027887/1027887]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls\n","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:14.826642Z","iopub.execute_input":"2022-11-21T12:35:14.827394Z","iopub.status.idle":"2022-11-21T12:35:15.992304Z","shell.execute_reply.started":"2022-11-21T12:35:14.827350Z","shell.execute_reply":"2022-11-21T12:35:15.990511Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"__notebook_source__.ipynb  prompts.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"prompts = pd.read_csv(\"prompts.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:15.999209Z","iopub.execute_input":"2022-11-21T12:35:16.000154Z","iopub.status.idle":"2022-11-21T12:35:16.038248Z","shell.execute_reply.started":"2022-11-21T12:35:16.000096Z","shell.execute_reply":"2022-11-21T12:35:16.037300Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"prompts.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:16.040708Z","iopub.execute_input":"2022-11-21T12:35:16.041085Z","iopub.status.idle":"2022-11-21T12:35:16.062393Z","shell.execute_reply.started":"2022-11-21T12:35:16.041050Z","shell.execute_reply":"2022-11-21T12:35:16.061282Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  A portrait photo of a kangaroo wearing an oran...   \n1           inmates with cow heads inside a jailcell   \n2  daguerrotype of a corgi astronaut on the moon,...   \n3  totem animal tribal chaman vodoo mask feather ...   \n4                                        p. cubensis   \n\n                                            raw_data  \n0  {\"image_uri\": \"PENDING\", \"modifiers\": [\"portra...  \n1  {\"image_uri\": \"PENDING\", \"modifiers\": [\"inmate...  \n2  {\"image_uri\": \"PENDING\", \"modifiers\": [\"daguer...  \n3  {\"image_uri\": \"PENDING\", \"modifiers\": [\"totem ...  \n4  {\"image_uri\": \"PENDING\", \"modifiers\": [\"p cube...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>raw_data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A portrait photo of a kangaroo wearing an oran...</td>\n      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"portra...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>inmates with cow heads inside a jailcell</td>\n      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"inmate...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>daguerrotype of a corgi astronaut on the moon,...</td>\n      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"daguer...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>totem animal tribal chaman vodoo mask feather ...</td>\n      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"totem ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>p. cubensis</td>\n      <td>{\"image_uri\": \"PENDING\", \"modifiers\": [\"p cube...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"prompts.loc[50,]","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:16.064140Z","iopub.execute_input":"2022-11-21T12:35:16.064833Z","iopub.status.idle":"2022-11-21T12:35:16.073838Z","shell.execute_reply.started":"2022-11-21T12:35:16.064796Z","shell.execute_reply":"2022-11-21T12:35:16.072556Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"prompt      A single dragon with half open wings breathing...\nraw_data    {\"image_uri\": \"PENDING\", \"modifiers\": [\"single...\nName: 50, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":">  I'm actually splitting this entire prompt based on space in a very similarly tokenizing but a very naive way and then finding out if the number of words are less than six number of tokens are less than six I'm then excluding them so I basically I want prompts where the number of take number of words is greater than six that's what I'm doing here and I'm calling this data frame as prompts underscore GT6 which refers to greater than 6.","metadata":{}},{"cell_type":"code","source":"prompts_gt6 = prompts.loc[prompts.prompt.str.split(' ').str.len() > 6]","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:16.075631Z","iopub.execute_input":"2022-11-21T12:35:16.076594Z","iopub.status.idle":"2022-11-21T12:35:16.093843Z","shell.execute_reply.started":"2022-11-21T12:35:16.076551Z","shell.execute_reply":"2022-11-21T12:35:16.092937Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"> At this point we're going to install a new library called aitextge exchange this is from minimaxir who is done excellent contribution to the text generation and image generation domain in terms of Open Source projects so now we have taken this aitextge-xn which is going to make it really really easy for us to do fine tuning of the GPT Neo model or even you can do gpt2 as well but I am currently doing a GPT Neo model then once we install the libraryaitextgenaitextgen aitextge","metadata":{}},{"cell_type":"code","source":"! pip install aitextgen  -q","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:16.095006Z","iopub.execute_input":"2022-11-21T12:35:16.099291Z","iopub.status.idle":"2022-11-21T12:35:33.263476Z","shell.execute_reply.started":"2022-11-21T12:35:16.099257Z","shell.execute_reply":"2022-11-21T12:35:33.262295Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"> Now we are going to specify what model we want to use and we are just specifying that from illusory a we want to use the GPT neo 125 million parameter model we can use different models as well and the I think they've got a bigger 350 million parameter model we can use that as well or if we want then take any gpt2 model from hugging phase model we can use that as well .","metadata":{}},{"cell_type":"code","source":"model=\"EleutherAI/gpt-neo-125M\"","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:33.265648Z","iopub.execute_input":"2022-11-21T12:35:33.266068Z","iopub.status.idle":"2022-11-21T12:35:33.273952Z","shell.execute_reply.started":"2022-11-21T12:35:33.266024Z","shell.execute_reply":"2022-11-21T12:35:33.272900Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"> we have to prepare the input dataset for this particular model which is AI to exchange so the way we are going to do it is we are going to keep only the text column and we're going to store that as a .txt file because aitextgen takes a Dos.txt file and then trains like fine-tunes the model so I'm removing the column and resetting the access and saving it as a file called input text cleaned and the column name the column that I want to save is prompt and we are not going to have any header we are not going to have index and at this point our training data set is ready for us to use .","metadata":{}},{"cell_type":"code","source":"prompts_gt6 = prompts_gt6.drop('raw_data', axis = 1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:33.275644Z","iopub.execute_input":"2022-11-21T12:35:33.275930Z","iopub.status.idle":"2022-11-21T12:35:33.287927Z","shell.execute_reply.started":"2022-11-21T12:35:33.275905Z","shell.execute_reply":"2022-11-21T12:35:33.287009Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"prompts_gt6.to_csv(\"input_text_cleaned.txt\", columns=[\"prompt\"], header=False, index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:33.294365Z","iopub.execute_input":"2022-11-21T12:35:33.294625Z","iopub.status.idle":"2022-11-21T12:35:33.306985Z","shell.execute_reply.started":"2022-11-21T12:35:33.294601Z","shell.execute_reply":"2022-11-21T12:35:33.306002Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"> now we're going to call from a text and token data set import token dataset and then take that into this so the attribute or the parameter line by line means do you want this entire document to be read as one text or do you want every line individual line to be read separately we have in our case we want every individual line to be read separately so we are seeing line by line is equal to true and then we are reading the latest hit and at this point from aitextgen exchange import a text in we are going to import the model so right now as you can see I'm importing the illusory as GPT Neo model.","metadata":{}},{"cell_type":"code","source":"from aitextgen.TokenDataset import TokenDataset","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:33.309834Z","iopub.execute_input":"2022-11-21T12:35:33.310136Z","iopub.status.idle":"2022-11-21T12:35:41.324843Z","shell.execute_reply.started":"2022-11-21T12:35:33.310112Z","shell.execute_reply":"2022-11-21T12:35:41.323818Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data = TokenDataset('./input_text_cleaned.txt', line_by_line=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:41.326566Z","iopub.execute_input":"2022-11-21T12:35:41.327478Z","iopub.status.idle":"2022-11-21T12:35:41.647212Z","shell.execute_reply.started":"2022-11-21T12:35:41.327437Z","shell.execute_reply":"2022-11-21T12:35:41.646328Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/896 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46b72ae85f8d4740b03583bc6a57f4b0"}},"metadata":{}}]},{"cell_type":"code","source":"from aitextgen import aitextgen","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:41.648805Z","iopub.execute_input":"2022-11-21T12:35:41.649473Z","iopub.status.idle":"2022-11-21T12:35:41.654472Z","shell.execute_reply.started":"2022-11-21T12:35:41.649434Z","shell.execute_reply":"2022-11-21T12:35:41.653448Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"ai = aitextgen(model = model,  to_gpu=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:35:41.655894Z","iopub.execute_input":"2022-11-21T12:35:41.656550Z","iopub.status.idle":"2022-11-21T12:36:55.144450Z","shell.execute_reply.started":"2022-11-21T12:35:41.656514Z","shell.execute_reply":"2022-11-21T12:36:55.143408Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json not found in cache or force_download set to True, downloading to /kaggle/working/aitextgen/tmpq8sm6o_2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/0.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42b17e092e004901bcbdcc4978553552"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json in cache at aitextgen/29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\ncreating metadata file for aitextgen/29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\nhttps://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /kaggle/working/aitextgen/tmpvjyzwxeg\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/502M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d153ce4d00ef4744ad26da52ec62f678"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/pytorch_model.bin in cache at aitextgen/b0ace3b93ace62067a246888f1e54e2d3ec20807d4d3e27ac602eef3b7091c0b.6525df88f1d5a2d33d95ce2458ef6af9658fe7d1393d6707e0e318779ccc68ff\ncreating metadata file for aitextgen/b0ace3b93ace62067a246888f1e54e2d3ec20807d4d3e27ac602eef3b7091c0b.6525df88f1d5a2d33d95ce2458ef6af9658fe7d1393d6707e0e318779ccc68ff\nhttps://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /kaggle/working/aitextgen/tmptm6jashf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccdad6796c4e4b8daa3e77f924d8623e"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json in cache at aitextgen/3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\ncreating metadata file for aitextgen/3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\nhttps://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /kaggle/working/aitextgen/tmp6fs1v4kt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"525d632fa7604b23855e7663fd53f077"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json in cache at aitextgen/08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\ncreating metadata file for aitextgen/08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\nhttps://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /kaggle/working/aitextgen/tmpk7nwxrd4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac7c32f983db4fdea3f75f52ff91290b"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt in cache at aitextgen/12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\ncreating metadata file for aitextgen/12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\nhttps://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /kaggle/working/aitextgen/tmplzm1vhjs\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"307a4f7620364026874e9bb5b3a56a78"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json in cache at aitextgen/6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\ncreating metadata file for aitextgen/6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\nloading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json from cache at aitextgen/08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\nloading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt from cache at aitextgen/12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\nloading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json from cache at aitextgen/6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\nloading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json from cache at aitextgen/3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\nhttps://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpa9eydu65\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/0.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e248850dea84aeaa096fc48022e8d67"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\ncreating metadata file for /root/.cache/huggingface/transformers/29380fef22a43cbfb3d3a6c8e2f4fd951459584d87c34e4621b30580a54aca84.f0f7ebddfc6e15a23ac33e7fa95cd8cca05edf87cc74f9e3be7905f538a59762\n","output_type":"stream"}]},{"cell_type":"code","source":"ai.train('input_text_cleaned.txt',\n         line_by_line=True,\n         from_cache=False,\n         num_steps=500,\n         generate_every=100,\n         save_every=500,\n         save_gdrive=False,\n         learning_rate=1e-3,\n         fp16=False,\n         batch_size=1, \n         )","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:36:55.146116Z","iopub.execute_input":"2022-11-21T12:36:55.146487Z","iopub.status.idle":"2022-11-21T12:40:53.536489Z","shell.execute_reply.started":"2022-11-21T12:36:55.146450Z","shell.execute_reply":"2022-11-21T12:40:53.534963Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/896 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"506acf477bdd41dd8cd403e4b4c7ffce"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:448: LightningDeprecationWarning: Setting `Trainer(gpus=-1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=-1)` instead.\n  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:259: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n  f\"The `Callback.{hook}` hook was deprecated in v1.6 and\"\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0237f2d628b44c89bcb7b13b79b3f6ff"}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[1m100 steps reached: generating sample texts.\u001b[0m\n==========\n a tallahah, in a rain and a rain and a rain and a sun in a rain and a sun and a sun and a sun and a sun and a sun and a low on the horizon, night, night, night, night, night, night, night, night, night, night, night, night, night, night, night, night, hard light, night, hard light, night, light, hard light, soft, soft, 3 d, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, soft, sharp focus, illustration, artstation, cgsociety, greg rutkowski, cgsociety, angular, greg rutkowski, greg rutkowski, greg rutkowski, greg rutkowski, greg rutkowski, greg rutkowski, greg rutkowski, g\n==========\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[1m200 steps reached: generating sample texts.\u001b[0m\n==========\n, trending on artstation, octane render, HDR, CGsociety, volumetric lighting\"\n\n==========\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[1m300 steps reached: generating sample texts.\u001b[0m\n==========\n\"Portrait of a lady in spring, painting by stanhope forbes, oil on canvas\"\n\n==========\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[1m400 steps reached: generating sample texts.\u001b[0m\n==========\n\n==========\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[1m500 steps reached: saving model to /trained_model\u001b[0m\n\u001b[1m500 steps reached: generating sample texts.\u001b[0m\n==========\nen whelan and Gary Houston, jungle nature, fruit trees, fruit trees, very beautiful art, fruit trees, very very very beautiful art, fruit trees, very very very beautiful art, tropical, fruit tropical, fruit tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical, tropical\n==========\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"ai.save()","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:40:53.543696Z","iopub.execute_input":"2022-11-21T12:40:53.544504Z","iopub.status.idle":"2022-11-21T12:40:54.341736Z","shell.execute_reply.started":"2022-11-21T12:40:53.544463Z","shell.execute_reply":"2022-11-21T12:40:54.340333Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"prompt_ai = aitextgen(model_folder = '.', to_gpu=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:40:54.343408Z","iopub.execute_input":"2022-11-21T12:40:54.344063Z","iopub.status.idle":"2022-11-21T12:40:57.244495Z","shell.execute_reply.started":"2022-11-21T12:40:54.344023Z","shell.execute_reply":"2022-11-21T12:40:57.243489Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(prompt_ai.generate(prompt = \"electronic device\"))","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:40:57.246061Z","iopub.execute_input":"2022-11-21T12:40:57.246635Z","iopub.status.idle":"2022-11-21T12:40:57.682828Z","shell.execute_reply.started":"2022-11-21T12:40:57.246595Z","shell.execute_reply":"2022-11-21T12:40:57.681688Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\u001b[1melectronic device\u001b[0m, digital art, overdetailed inksorianstyle, overdetailed inksorianstyle, overdetailed inksorianstyle, segsociety, unreal engine 5, radiant light, detailed and intricate environment\"\n\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"print(prompt_ai.generate_one(prompt = \"astronaut \"))","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:40:57.684362Z","iopub.execute_input":"2022-11-21T12:40:57.685175Z","iopub.status.idle":"2022-11-21T12:40:57.953164Z","shell.execute_reply.started":"2022-11-21T12:40:57.685127Z","shell.execute_reply":"2022-11-21T12:40:57.952241Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"astronaut ernst haeckel,ernst haeckel,ernst haeckel,ernst haeckel, hd\"\n\n","output_type":"stream"}]},{"cell_type":"code","source":"ai.save_for_upload('sd-prompt-generator-gpt-neo')","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:40:57.954709Z","iopub.execute_input":"2022-11-21T12:40:57.955145Z","iopub.status.idle":"2022-11-21T12:40:59.556319Z","shell.execute_reply.started":"2022-11-21T12:40:57.955108Z","shell.execute_reply":"2022-11-21T12:40:59.555291Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"tokenizer config file saved in sd-prompt-generator-gpt-neo/tokenizer_config.json\nSpecial tokens file saved in sd-prompt-generator-gpt-neo/special_tokens_map.json\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:40:59.557661Z","iopub.execute_input":"2022-11-21T12:40:59.558041Z","iopub.status.idle":"2022-11-21T12:41:00.107076Z","shell.execute_reply.started":"2022-11-21T12:40:59.558004Z","shell.execute_reply":"2022-11-21T12:41:00.106103Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edacf64df577402eaf90d440442be592"}},"metadata":{}}]},{"cell_type":"code","source":"!sudo apt-get install git-lfs","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:46:13.310077Z","iopub.execute_input":"2022-11-21T12:46:13.311205Z","iopub.status.idle":"2022-11-21T12:46:15.693603Z","shell.execute_reply.started":"2022-11-21T12:46:13.311156Z","shell.execute_reply":"2022-11-21T12:46:15.692401Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ngit-lfs is already the newest version (2.9.2-1).\n0 upgraded, 0 newly installed, 0 to remove and 93 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install --upgrade huggingface_hub -q","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:46:22.036939Z","iopub.execute_input":"2022-11-21T12:46:22.038219Z","iopub.status.idle":"2022-11-21T12:46:31.403914Z","shell.execute_reply.started":"2022-11-21T12:46:22.038160Z","shell.execute_reply":"2022-11-21T12:46:31.402721Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:46:36.140501Z","iopub.execute_input":"2022-11-21T12:46:36.140935Z","iopub.status.idle":"2022-11-21T12:46:36.146140Z","shell.execute_reply.started":"2022-11-21T12:46:36.140895Z","shell.execute_reply":"2022-11-21T12:46:36.145177Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:46:40.630767Z","iopub.execute_input":"2022-11-21T12:46:40.631694Z","iopub.status.idle":"2022-11-21T12:46:40.637013Z","shell.execute_reply.started":"2022-11-21T12:46:40.631658Z","shell.execute_reply":"2022-11-21T12:46:40.635005Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-11-21T12:46:45.105307Z","iopub.execute_input":"2022-11-21T12:46:45.105715Z","iopub.status.idle":"2022-11-21T12:46:48.290102Z","shell.execute_reply.started":"2022-11-21T12:46:45.105680Z","shell.execute_reply":"2022-11-21T12:46:48.288459Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mResponse\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mendpoint_name\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moptional\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/RajaSi/sd-prompt-generator-gpt-neo-gn/preupload/main","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3914622166.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpath_in_repo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RajaSi/sd-prompt-generator-gpt-neo-gn\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mRaises\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHFValidationError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0man\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \"\"\"\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mupload_folder\u001b[0;34m(self, repo_id, folder_path, path_in_repo, commit_message, commit_description, token, repo_type, revision, create_pr, parent_commit, allow_patterns, ignore_patterns)\u001b[0m\n\u001b[1;32m   2391\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mcreate_pr\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcommit\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mfail\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mparent_commit\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2392\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mcreate_pr\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpull\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mparent_commit\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2393\u001b[0;31m                 \u001b[0mSpecifying\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mparent_commit\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mensures\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrepo\u001b[0m \u001b[0mhas\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchanged\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mcommitting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mchanges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2394\u001b[0m                 \u001b[0mespecially\u001b[0m \u001b[0museful\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrepo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcommitted\u001b[0m \u001b[0mto\u001b[0m \u001b[0mconcurrently\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2395\u001b[0m         \"\"\"\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mRaises\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHFValidationError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0man\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \"\"\"\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit)\u001b[0m\n\u001b[1;32m   2035\u001b[0m         \u001b[0;34m...\u001b[0m         \u001b[0mpath_in_repo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"remote/file/path.h5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;34m...\u001b[0m         \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"username/my-dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m         \u001b[0;34m...\u001b[0m         \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;34m...\u001b[0m         \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"my_token\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m         ...     )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mRaises\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHFValidationError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0man\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \"\"\"\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/_commit_api.py\u001b[0m in \u001b[0;36mfetch_upload_modes\u001b[0;34m(additions, repo_type, repo_id, token, revision, endpoint, create_pr)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         )\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{operation.path_in_repo}: Upload successful\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Error-Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"RevisionNotFound\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             message = (\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: 5N-2-EvFWVfgUtLcNXnR5)\n\nRepository Not Found for url: https://huggingface.co/api/models/RajaSi/sd-prompt-generator-gpt-neo-gn/preupload/main.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf the repo is private, make sure you are authenticated.\nUnauthorized\nNote: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case."],"ename":"RepositoryNotFoundError","evalue":"401 Client Error. (Request ID: 5N-2-EvFWVfgUtLcNXnR5)\n\nRepository Not Found for url: https://huggingface.co/api/models/RajaSi/sd-prompt-generator-gpt-neo-gn/preupload/main.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf the repo is private, make sure you are authenticated.\nUnauthorized\nNote: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case.","output_type":"error"}]}]}